{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_transforms = {\n",
    "    'carspotting_batch': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'carspotting': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        #transforms.ToPILImage(),\n",
    "    ]),\n",
    "    'to_tensor': transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{864, 751, 817, 627, 468, 734}\n"
     ]
    }
   ],
   "source": [
    "#print(model_conv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#from skimage import io\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "model_conv = models.vgg19(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "#model_conv.to(device)\n",
    "\n",
    "#allowed_indices = [436, 705, 751, 817, 864, 627, 734, 468]\n",
    "allowed_indices = set([751, 817, 864, 627, 734, 468])\n",
    "print(allowed_indices)\n",
    "transform = data_transforms['carspotting']\n",
    "\n",
    "carpics= glob.glob('./data/**/*.jpg', recursive=True)\n",
    "trashdir = './cleaned_data/trash/'\n",
    "cardir = './cleaned_data/car/'\n",
    "\n",
    "if not os.path.exists(trashdir):\n",
    "    os.makedirs(trashdir)\n",
    "    \n",
    "if not os.path.exists(cardir):\n",
    "    os.makedirs(cardir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-21-7d1c0fc7bdce>(19)<module>()\n",
      "-> b, c, h, w = transformed_image.shape\n",
      "> <ipython-input-21-7d1c0fc7bdce>(20)<module>()\n",
      "-> if c != 3:\n",
      "> <ipython-input-21-7d1c0fc7bdce>(18)<module>()\n",
      "-> import pdb; pdb.set_trace()\n",
      "torch.Size([1, 3, 224, 224])\n",
      "*** SyntaxError: invalid syntax\n",
      "> <ipython-input-21-7d1c0fc7bdce>(19)<module>()\n",
      "-> b, c, h, w = transformed_image.shape\n",
      "1\n",
      "3\n",
      "224\n",
      "224\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7d1c0fc7bdce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-7d1c0fc7bdce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "steps = len(carpics) // batch_size\n",
    "count = 0\n",
    "\n",
    "for i in range(steps+1):\n",
    "    if i != steps:\n",
    "        image_batch = carpics[i*batch_size: (i+1)*batch_size]\n",
    "    elif i == steps:\n",
    "        image_batch = carpics[i*batch_size: len(carpics)]\n",
    "    \n",
    "    #transform = data_transforms['carspotting_batch']\n",
    "    #input_batch = np.zeros((len(image_batch), 3, 224, 224), dtype=np.float64)\n",
    "    input_batch = []\n",
    "    for i, image_path in enumerate(image_batch):\n",
    "        pil_image = Image.open(image_path)\n",
    "        transformed_image = transform(pil_image)\n",
    "        transformed_image.unsqueeze_(0)\n",
    "        import pdb; pdb.set_trace()\n",
    "        b, c, h, w = transformed_image.shape\n",
    "        if not (c == 3 and h == 224 and w == 224):\n",
    "            continue\n",
    "        input_batch.append(transformed_image)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        #input_batch[i, :, :, :] = transformed_image_tensor\n",
    "    if len(input_batch) == 0:\n",
    "        continue\n",
    "    input = torch.cat(input_batch)\n",
    "    #input.to(device)\n",
    "    #import pdb; pdb.set_trace()\n",
    "        \n",
    "    try:\n",
    "        output = model_conv(input)\n",
    "    except RuntimeError:\n",
    "        continue\n",
    "        \n",
    "    #import pdb; pdb.set_trace()\n",
    "        \n",
    "    sorted_top_5_labels = np.argsort(output)\n",
    "    sorted_top_5_labels = sorted_top_5_labels[:, -5:]\n",
    "    \n",
    "    for i, image in enumerate(image_batch):\n",
    "        trash = False\n",
    "        car = False\n",
    "        \n",
    "        #import pdb; pdb.set_trace()\n",
    "\n",
    "        if len(set(sorted_top_5_labels[i, :].numpy()).intersection(allowed_indices)) != 0:\n",
    "            car = True\n",
    "        else:\n",
    "            trash = True\n",
    "            \n",
    "        file = image\n",
    "\n",
    "        if trash:\n",
    "            outDir = trashdir + '/'.join(file.split('/')[2:-1])\n",
    "            if not os.path.exists(outDir):\n",
    "                os.makedirs(outDir)\n",
    "\n",
    "            copyfile(file, trashdir+'/'.join(file.split('/')[2:]))\n",
    "\n",
    "        if car:\n",
    "            outDir = cardir + '/'.join(file.split('/')[2:-1])\n",
    "            if not os.path.exists(outDir):\n",
    "                os.makedirs(outDir)\n",
    "\n",
    "            copyfile(file, cardir+'/'.join(file.split('/')[2:]))\n",
    "    \n",
    "    count += 1\n",
    "    print(count*batch_size)\n",
    "        \n",
    "\"\"\"\n",
    "for i in range(len(carpics)):\n",
    "    import pdb; pdb.set_trace()\n",
    "    image_path = carpics[i]\n",
    "    pil_image = Image.open(image_path)\n",
    "    transformed_image = transform(pil_image)\n",
    "    transformed_image.unsqueeze_(0)\n",
    "    \n",
    "    try:\n",
    "        output = model_conv(transformed_image)\n",
    "    except RuntimeError:\n",
    "        continue\n",
    "    \n",
    "    sorted_top_5_labels = np.argsort(output)[:, ::-1][:5]\n",
    "    \n",
    "    trash = False\n",
    "    car = False\n",
    "    \n",
    "    if len(set(sorted_top_5_labels).intersection(allowed_indices)) != 0:\n",
    "        car = True\n",
    "    else:\n",
    "        trash = True\n",
    "        \n",
    "    import pdb; pdb.set_trace()\n",
    "        \n",
    "    if trash:\n",
    "        outDir = trashdir + '/'.join(file.split('/')[3:-1])\n",
    "        if not os.path.exists(outDir):\n",
    "            os.makedirs(outDir)\n",
    "\n",
    "        copyfile(file, trashdir+'/'.join(file.split('/')[3:]))\n",
    "\n",
    "    if car:\n",
    "        outDir = cardir + '/'.join(file.split('/')[3:-1])\n",
    "        if not os.path.exists(outDir):\n",
    "            os.makedirs(outDir)\n",
    "\n",
    "        copyfile(file, cardir+'/'.join(file.split('/')[3:]))\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_conv)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#from skimage import io\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import glob\n",
    "from shutil import copyfile\n",
    "import os\n",
    "\n",
    "cardir = './cleaned_data/car/'\n",
    "audidir = './cleaned_data/audi/'\n",
    "\n",
    "carpics= glob.glob('./cleaned_data/car/**/*.jpg', recursive=True)\n",
    "\n",
    "for file in carpics:\n",
    "    if \"audi\" in str(file).lower():\n",
    "        import pdb; pdb.set_trace()\n",
    "        outDir = audidir + '/'.join(file.split('/')[2:-1])\n",
    "        if not os.path.exists(outDir):\n",
    "            os.makedirs(outDir)\n",
    "\n",
    "        copyfile(file, audidir+'/'.join(file.split('/')[2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
